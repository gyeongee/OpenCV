{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7ba111-8502-42bf-a5be-fce9a3fe4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8640f34-2f4c-466e-8339-64a7281c1984",
   "metadata": {},
   "source": [
    "### 좋은 매칭점 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f30e8fb0-79cc-4f49-b806-bf91c80098d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches:18/127, min:24.00, max:78.00, threshold:34.80\n"
     ]
    }
   ],
   "source": [
    "img1 = cv2.imread('./img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('./img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB 검출\n",
    "detector = cv2.ORB_create()\n",
    "kp1, des1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, des2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BF 매칭\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(des1, des2)\n",
    "\n",
    "# 개선 작업\n",
    "matches = sorted(matches, key=lambda x: x.distance)  # 거리가 작은 순으로 정렬\n",
    "min_dist, max_dist = matches[0].distance, matches[-1].distance\n",
    "\n",
    "# 최소 거리보다 ratio만큼 더 긴 매칭된 결과만 보겠다\n",
    "ratio = 0.2    # ratio 조절 해가며 keypoints matching 결과 조정 클수록 더 많이 본다\n",
    "good_thresh = (max_dist - min_dist) * ratio + min_dist\n",
    "\n",
    "# 잘된 매칭점\n",
    "good_matches = [i for i in matches if i.distance < good_thresh]\n",
    "# good_matches = []\n",
    "# for i in matches:\n",
    "#     if i.distance < good_thresh:\n",
    "#         good_matches.append(i)\n",
    "print('matches:%d/%d, min:%.2f, max:%.2f, threshold:%.2f'\n",
    "      %(len(good_matches),len(matches),min_dist,max_dist,good_thresh))\n",
    "\n",
    "# 매칭 선 그리기\n",
    "res1 = cv2.drawMatches(img1, kp1, img2, kp2, matches, None,\n",
    "                      flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "res2 = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None,\n",
    "                      flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('bad',res1)\n",
    "cv2.imshow('good',res2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a1677a-61f5-46ff-b6ad-e5a2c959a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn 개선\n",
    "img1 = cv2.imread('./img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('./img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB 검출\n",
    "detector = cv2.ORB_create()\n",
    "kp1, des1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, des2 = detector.detectAndCompute(gray2, None)\n",
    "\n",
    "# BF 매칭\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\n",
    "matches = matcher.knnMatch(des1, des2, 2)\n",
    "\n",
    "# 개선\n",
    "ratio = 0.7\n",
    "good_matches = [first for first,second in matches \n",
    "                if first.distance < second.distance*ratio]\n",
    "# good_matches = []\n",
    "# for first, second in matches:\n",
    "#     if first.distance < second.distance * ratio:\n",
    "#         good_matches.append(first)\n",
    "res1 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, matches, None,\n",
    "                       flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "res2 = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None,\n",
    "                       flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "                       \n",
    "cv2.imshow('bad', res1)\n",
    "cv2.imshow('good', res2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61098b-9f44-4cdb-a11e-7690a10574af",
   "metadata": {},
   "source": [
    "## 매칭 영역 원근 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0261ff3a-0ed0-4b51-aca0-cc21d66d68ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good:23/500\n"
     ]
    }
   ],
   "source": [
    "img1 = cv2.imread('./img/taekwonv1.jpg')\n",
    "img2 = cv2.imread('./img/figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB, BF-Hamming, knnMatch\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\n",
    "matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "\n",
    "# 이웃거리 비율\n",
    "ratio = 0.75\n",
    "good_matches = [first for first,second in matches\n",
    "                if first.distance<second.distance*ratio]\n",
    "print('good:%d/%d'%(len(good_matches), len(matches)))\n",
    "\n",
    "src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "\n",
    "# 원근 변환 행렬 구하기\n",
    "mtrx, mask = cv2.findHomography(src_pts, dst_pts)\n",
    "\n",
    "h, w = img1.shape[:2]\n",
    "pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ]) # 각 꼭지점 좌표\n",
    "\n",
    "dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "res = cv2.drawMatches(img1,kp1,img2,kp2,good_matches,None,\n",
    "                      flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "cv2.imshow('homography', res)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee9b72-2038-4496-a810-adf5fb44ba8b",
   "metadata": {},
   "source": [
    "#### Video 읽기 및 다루기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b4fc639-9fd1-43d3-9807-82c089d5f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './img/big_buck.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if cap.isOpened():\n",
    "    while True:     # True 동안 다음 프레임 읽어오기\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:  # 영상 끝!!\n",
    "            break\n",
    "        cv2.imshow('video', frame)\n",
    "        if cv2.waitKey(1) & 0xff == 27:    # 25ms 지연\n",
    "            break\n",
    "else:\n",
    "    print('no video')\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5617a1e-2142-4483-b669-530c51dac27e",
   "metadata": {},
   "source": [
    "## 배경 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f06dbe2-c1e9-4f0d-b1bf-9e9209f5c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)   # 영상 프레임 수 구하기\n",
    "delay = int(1000/fps)   # waitKey에 들어갈 숫자\n",
    "\n",
    "# 배경 제거 객체 생성\n",
    "fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # background subtractor mask\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('bgsubtract',fgmask)\n",
    "    if cv2.waitKey(delay) & 0xff == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d1e5327-3b5d-45c5-a088-6457a01b3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = int(1000/fps)\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('bgsub', fgmask)\n",
    "    if cv2.waitKey(delay) & 0xff == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c020dad7-3d96-40fb-a227-7b4ccf9df5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)   # 영상 프레임 수 구하기\n",
    "delay = int(1000/fps)   # waitKey에 들어갈 숫자\n",
    "\n",
    "# 배경 제거 객체 생성\n",
    "fgbg = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "fgbg2 = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # background subtractor mask\n",
    "    fgmask1 = fgbg.apply(frame)\n",
    "    fgmask2 = fgbg2.apply(frame)\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('MOG',fgmask1)\n",
    "    cv2.imshow('MOG2',fgmask2)\n",
    "    if cv2.waitKey(delay) & 0xff == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339589a6-cd7c-43d8-ac55-604689771347",
   "metadata": {},
   "source": [
    "### Optical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6370daf-6874-4363-8a85-fdceddac313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = int(1000/fps)\n",
    "\n",
    "color = np.random.randint(0,255,(200,3))\n",
    "lines = None\n",
    "prevImg = None\n",
    "# 반복 중지 조건\n",
    "termcriteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # 맨 첫 프레임(영상의 시작)\n",
    "    if prevImg is None:\n",
    "        prevImg = gray\n",
    "        # 추적선을 그릴 이미지를 프레임 크기랑 같게 생성\n",
    "        lines = np.zeros_like(frame)\n",
    "        # 추적 시작을 위한 Keypoint 검출\n",
    "        prevPt = cv2.goodFeaturesToTrack(prevImg, 200, 0.01, 10)\n",
    "    else:\n",
    "        nextImg = gray\n",
    "        nextPt, status, err = cv2.calcOpticalFlowPyrLK(prevImg, nextImg,\n",
    "                                                       prevPt, None,\n",
    "                                                       criteria=termcriteria)\n",
    "        # previous, next 서로 대응점이 있는 keypoint\n",
    "        prevMv = prevPt[status==1]\n",
    "        nextMv = nextPt[status==1]\n",
    "        for i, (p,n) in enumerate(zip(prevMv,nextMv)):\n",
    "            px,py=p.ravel()\n",
    "            nx,ny=n.ravel()\n",
    "            px,py,nx,ny = int(px),int(py),int(nx),int(ny)\n",
    "            # 이전, 다음 keypoint 연결 선\n",
    "            cv2.line(lines,(px,py),(nx,ny), color[i].tolist(),2)\n",
    "            # 새로운 keypoint에 점\n",
    "            cv2.circle(img_draw, (nx,ny), 2, color[i].tolist(), -1)\n",
    "        img_draw = cv2.add(img_draw, lines)\n",
    "        prevImg = nextImg\n",
    "        prevPt = nextMv.reshape(-1,1,2)\n",
    "    cv2.imshow('OpticalFlow-LK', img_draw)\n",
    "    key = cv2.waitKey(delay)\n",
    "    if key == 27:    #esc\n",
    "        break\n",
    "    elif key == 8:   # backspace\n",
    "        prevImg = None\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a2dcec-8029-487e-ad36-e72281742c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow의 결과 그리기 함수\n",
    "def drawFlow(img, flow, step=16):\n",
    "    h,w = img.shape[:2]\n",
    "    idx_y,idx_x = np.mgrid[step/2:h:step, step/2:w:step].astype(np.int32)\n",
    "    indices = np.stack((idx_x,idx_y), axis=-1).reshape(-1,2) #(x,y)\n",
    "    \n",
    "    for x,y in indices:\n",
    "        # Grid의 Index위치 점 찍기\n",
    "        cv2.circle(img, (x,y), 1, (0,255,0), -1)\n",
    "        # Grid Index의 Flow 결과값 (이동 거리)\n",
    "        dx, dy = flow[y,x].astype(np.int32)\n",
    "        # (x,y), (dx,dy) 선\n",
    "        cv2.line(img, (x,y), (x+dx,y+dy), (0,255,0), 2, cv2.LINE_AA)\n",
    "\n",
    "prev = None\n",
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = int(1000/fps)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    if prev is None:\n",
    "        prev = gray\n",
    "    else:\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev,gray,None,\n",
    "                                            0.5,3,15,3,5,1.1,\n",
    "                                            cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "        drawFlow(frame, flow)\n",
    "        prev = gray\n",
    "    cv2.imshow('OpticalFlow-GF', frame)\n",
    "    if cv2.waitKey(delay) & 0xff == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afb0c2-832a-4a3c-b6ae-29649a8e06b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da75fbc-0821-44ca-9ac0-8bcc4fdeed98",
   "metadata": {},
   "source": [
    "## Tracking API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0b2d0c-7502-4ef6-a661-a7d0c1840cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers = [cv2.TrackerMIL_create, cv2.TrackerKCF_create, cv2.TrackerCSRT_create]\n",
    "trackerIdx = 0\n",
    "tracker = None\n",
    "isFirst = True\n",
    "video_src = './img/highway.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_src)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "delay = int(1000/fps)\n",
    "win_name = 'Tracking APIs'\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    img_draw = frame.copy()\n",
    "    if tracker is None:\n",
    "        cv2.putText(img_draw, 'Press the Space button', (100,80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,255), 2, cv2.LINE_AA)\n",
    "    else: \n",
    "        ok, bbox = tracker.update(frame)\n",
    "        (x,y,w,h) = bbox\n",
    "        if ok:\n",
    "            cv2.rectangle(img_draw, (int(x),int(y)),(int(x+w),int(y+h)),\n",
    "                          (0,255,0), 2, 1)\n",
    "        else:\n",
    "            cv2.putText(img_draw, 'Tracking Fail!', (100,80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,255), 2, cv2.LINE_AA)\n",
    "    trackerName = tracker.__class__.__name__\n",
    "    cv2.putText(img_draw, str(trackerIdx)+':'+trackerName, (100,20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,255,0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(win_name, img_draw)\n",
    "    key = cv2.waitKey(delay) & 0xff # == 27 ← ord('esc')\n",
    "    \n",
    "    # 스페이스바, 숫자 0,1,2 비디오 재생 중 누르는 키보드 이벤트\n",
    "    if key == ord(' ') or (video_src != 0 and isFirst):\n",
    "        isFirst = False\n",
    "        roi = cv2.selectROI(win_name, frame, False) # 초기 객체 위치\n",
    "        if roi[2] and roi[3]:\n",
    "            tracker = trackers[trackerIdx]()    # 트랙커 객체 생성\n",
    "            isInit = tracker.init(frame, roi)\n",
    "\n",
    "    elif key in range(48,51):\n",
    "        trackerIdx = key-48  # key 48, 49, 50 → 0, 1, 2\n",
    "        if bbox is not None:\n",
    "            tracker = trackers[trackerIdx]()\n",
    "            isInit = tracker.init(frame, bbox)\n",
    "\n",
    "    elif key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f69128-aaae-4850-adfa-2b061329c3fe",
   "metadata": {},
   "source": [
    "### HOG 디스크립터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9085e8af-514d-40bb-b446-53c5d9e70750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default 디텍터를 위한 객체 생성, 설정\n",
    "hogdef = cv2.HOGDescriptor()\n",
    "hogdef.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# dailer 디텍터 위한 객체 생성, 설정 / winsize, blocksize, bin(x축 나오는 갯수)\n",
    "hogdaim = cv2.HOGDescriptor((48,96),(16,16),(8,8),(8,8),9)\n",
    "hogdaim.setSVMDetector(cv2.HOGDescriptor_getDaimlerPeopleDetector())\n",
    "\n",
    "cap = cv2.VideoCapture('./img/walking.avi')\n",
    "mode = True\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    elif ret:\n",
    "        if mode:\n",
    "            # default로 검출\n",
    "            found, _ = hogdef.detectMultiScale(img)\n",
    "            for (x,y,w,h) in found:\n",
    "                cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,255))\n",
    "        else:\n",
    "            # daimler로 검출\n",
    "            found, _ = hogdaim.detectMultiScale(img)\n",
    "            for (x,y,w,h) in found:\n",
    "                cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0))\n",
    "        cv2.putText(img, 'Detector:%s'%('Default'if mode else 'Diamler'),\n",
    "                    (10,50), cv2.FONT_HERSHEY_DUPLEX,1,(0,255,0),1)\n",
    "        cv2.imshow('walk', img)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "        elif key == ord(' '):\n",
    "            mode = not mode\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f99d3-8762-49c2-801b-9bb0e37f6053",
   "metadata": {},
   "outputs": [],
   "source": [
    "'./weights/파일명'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "186cc33e-be39-4fba-8faa-42e89ae5168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('./img/people.jpg')\n",
    "\n",
    "image_resized = cv2.resize(image, (755,500))\n",
    "\n",
    "cascade_face_detector = cv2.CascadeClassifier('./weights/haarcascade_frontalface_default.xml')\n",
    "face_detections = cascade_face_detector.detectMultiScale(image_resize,\n",
    "                                                        scaleFactor=2,\n",
    "                                                        minNeighbors=4)\n",
    "# [(x,y,w,h), (x,y,w,h), (x,y,w,h), (x,y,w,h), (x,y,w,h),(x,y,w,h)]\n",
    "\n",
    "for (x,y,w,h) in face_detections:\n",
    "    cv2.rectangle(image_resized, (x,y), (x+w,y+h), (0,255,0),2)\n",
    "\n",
    "cv2.imshow('people', image_resized)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fe803cc-35ea-478d-a76c-5c543ca4338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./img/people.jpg')\n",
    "img_resize = cv2.resize(img, (755,500))\n",
    "\n",
    "cascade_eye_detector = cv2.CascadeClassifier('./weights/haarcascade_eye.xml')\n",
    "eye_detections = cascade_eye_detector.detectMultiScale(img_resize, \n",
    "                                                      scaleFactor=1.05,\n",
    "                                                      minNeighbors=6,\n",
    "                                                      minSize=(10,10),\n",
    "                                                      maxSize=(30,30))\n",
    "\n",
    "for (x,y,w,h) in eye_detections:\n",
    "    cv2.rectangle(img_resize, (x,y), (x+w,y+h), (0,255,255), 2)\n",
    "\n",
    "cv2.imshow('eye', img_resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "415fdc1e-2ed4-4dcf-8b09-db49a3c1c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./img/people.jpg')\n",
    "img_resize = cv2.resize(img, (755,500))\n",
    "\n",
    "cascade_face_detector = cv2.CascadeClassifier('./weights/haarcascade_frontalface_default.xml')\n",
    "cascade_eye_detector = cv2.CascadeClassifier('./weights/haarcascade_eye.xml')\n",
    "\n",
    "face_detections = cascade_face_detector.detectMultiScale(img_resize,\n",
    "                                                         scaleFactor=2)\n",
    "eye_detections = cascade_eye_detector.detectMultiScale(img_resize, \n",
    "                                                      scaleFactor=1.05,\n",
    "                                                      minNeighbors=6,\n",
    "                                                      minSize=(10,10),\n",
    "                                                      maxSize=(30,30))\n",
    "for (x,y,w,h) in face_detections:\n",
    "    cv2.rectangle(img_resize, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "\n",
    "for (x,y,w,h) in eye_detections:\n",
    "    cv2.rectangle(img_resize, (x,y), (x+w,y+h), (0,255,255), 2)\n",
    "\n",
    "cv2.imshow('all', img_resize)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df18b0c-2edf-48da-bfd8-969ff547c47d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a28bd489-dfe8-4735-9f18-25dc16cabc6e",
   "metadata": {},
   "source": [
    "### 얼굴 인식 LBPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6d60750-c34a-47e4-9cf5-6d082ce11dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (11.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be69b3c7-8ce3-465c-92a4-28c74eb28a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07a2a929-7435-405b-ae92-866742fb5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_path = './img/yalefaces/train'\n",
    "\n",
    "all_train_img_name = os.listdir(train_image_path)\n",
    "# ['subject01.centerlight.gif',\n",
    "#  'subject01.glasses.gif',\n",
    "#  'subject01.happy.gif',\n",
    "#  'subject01.leftlight.gif',...]\n",
    "all_train_img_path = []\n",
    "for i in all_train_img_name:\n",
    "    new_path = train_image_path + '/' + i\n",
    "    all_train_img_path.append(new_path)\n",
    "# ['./img/yalefaces/train/subject01.centerlight.gif',\n",
    "#  './img/yalefaces/train/subject01.glasses.gif',\n",
    "#  './img/yalefaces/train/subject01.happy.gif',\n",
    "#  './img/yalefaces/train/subject01.leftlight.gif',...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac3deb9-7ffd-4cc7-bc3e-6492a78e3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_preprocessed_image_data(image_paths):\n",
    "    faces_of_image = []\n",
    "    ids_of_image = []\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path).convert('L')  # cv2.cvtColor(cv2.COLOR_BGR2GRAY)\n",
    "        # to Grayscale\n",
    "        image_np = np.array(image, 'uint8')\n",
    "        image_file_name = os.path.split(image_path)[1]   # subject01.centerlight.gif\n",
    "        id_of_image = int(image_file_name.split('.')[0].replace('subject',''))    #int(01) == 1\n",
    "        ids_of_image.append(id_of_image)\n",
    "        faces_of_image.append(image_np)\n",
    "    return np.array(ids_of_image), faces_of_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51ff2a31-b348-4f4e-9c39-5db34eb84edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_of_train_image, faces_of_train_image = fetch_preprocessed_image_data(all_train_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23b3888f-935c-4a3c-9986-34de91a6e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "original_image = faces_of_train_image[6]\n",
    "# LBPH 적용\n",
    "lbp_image = local_binary_pattern(original_image, 8*3, 3)\n",
    "# 정규화 0~255\n",
    "lbp_image = ((lbp_image - lbp_image.min())/(lbp_image.max()-lbp_image.min())) * 255\n",
    "# 다시 이미지로\n",
    "lbp_image = lbp_image.astype(np.uint8)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd073734-1608-4776-95c1-eda12e761103",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('or',original_image)\n",
    "cv2.imshow('xor',lbp_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b26296cd-1a8f-42a8-b798-0056a4108a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 생성\n",
    "lbph_recognizer = cv2.face.LBPHFaceRecognizer_create(radius=5,neighbors=15,\n",
    "                                                    grid_x=10,grid_y=10)\n",
    "lbph_recognizer.train(faces_of_train_image, ids_of_train_image)\n",
    "# lbph_recognizer.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0dc9abaa-bbe9-425b-ba04-0a6781cf6399",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = './img/yalefaces/test'\n",
    "# 위에서 진행한 train과 동일한 내용\n",
    "all_test_image_file_name = os.listdir(test_image_path)\n",
    "all_test_image_paths = [os.path.join(test_image_path,f) for f in all_test_image_file_name]\n",
    "ids_of_test_image, faces_of_test_image = fetch_preprocessed_image_data(all_test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37b16fef-c4c9-48c8-8468-779daffb56b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 139.49445435935314)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_pred, confidence_pred = lbph_recognizer.predict(faces_of_test_image[0])\n",
    "id_pred, confidence_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1e03f34-e9b7-4865-99fb-3a720668b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_pred = []\n",
    "# Prediction\n",
    "for face_of_test_image in faces_of_test_image:\n",
    "    # face_of_test_image ← 이미지에 대해 예측한 Y값 (몇번 사람인지)\n",
    "    id_pred, _ = lbph_recognizer.predict(face_of_test_image)\n",
    "    ids_pred.append(id_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a711a203-9452-4c07-8016-0a00669c87da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0] 숫자 바꿔가며 보시면 됩니다!!\n",
    "\n",
    "cv2.putText(faces_of_test_image[1], f'Pred: {str(id_pred)}', (5, 30), cv2.FONT_HERSHEY_PLAIN, 1.1, (0, 255, 0))\n",
    "cv2.putText(faces_of_test_image[1], f'True: {str(ids_of_test_image[0])}', (5, 60), cv2.FONT_HERSHEY_PLAIN, 1.1, (0,255,0))\n",
    "\n",
    "cv2.imshow('test',faces_of_test_image[1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48b418a1-69c6-4143-bd8c-5a248515d17e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfba55bd-d05e-4e36-a3fd-c44d835ee0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(ids_of_test_image, ids_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84040da-11e8-4ab9-ba38-c6ccdea1bf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
